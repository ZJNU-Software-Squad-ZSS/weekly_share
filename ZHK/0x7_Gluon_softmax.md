## 0x7_Gluon_softmax

其实 softmax regression 和 linear regression 在代码上大同小异，虽然原理不同，但是在代码的结构上是完全一样的，或者也可以说，linear regression 麻雀虽小，五脏俱全。但是由于学习前的一些准备不是很充分，所以这次对softmax的记录出来的晚了些， scratch 和 gluon 两个版本的代码得等后续有空再补。因此，除了代码外，还想稍微记录一下原理。

#### softmax回归

softmax的输出是多个、离散的量，所以他更适合输出一些像图像类别那样的离散值。

*𝑜*1=*𝑥*1*𝑤*11+*𝑥*2*𝑤*21+*𝑥*3*𝑤*31+*𝑥*4*𝑤*41+*𝑏*1,

*𝑜*2=*𝑥*1*𝑤*12+*𝑥*2*𝑤*22+*𝑥*3*𝑤*32+*𝑥*4*𝑤*42+*𝑏*2,

*𝑜*3=*𝑥*1*𝑤*13+*𝑥*2*𝑤*23+*𝑥*3*𝑤*33+*𝑥*4*𝑤*43+*𝑏*3.

假设是三输出四输入

首先一点，softmax输出层是全连接层

既然分类问题需要得到离散的预测输出，一个简单的办法是将输出值oi当作预测类别是 i 的置信度，并将值最大的输出所对应的类作为预测输出。

然而，直接使用输出层的输出有两个问题。一方面，由于输出层的输出值的范围不确定，我们难以直观上判断这些值的意义。另一方面，由于真实标签是离散值，这些离散值与不确定范围的输出值之间的误差难以衡量。

所以，就有了softmax 。

它通过下式将输出值变换成值为正且和为1的概率分布：

*𝑦*̂1,*𝑦*̂2,*𝑦*̂3=softmax(*𝑜*1,*𝑜*2,*𝑜*3)

其中

*𝑦*̂1=exp(*𝑜*1) / ∑3*𝑖*=1exp(*𝑜* *𝑖*)  ,  *𝑦 ̂2=exp(*𝑜*2)∑3*𝑖*=1  /  exp(*𝑜 *𝑖*),     *𝑦*̂3=exp(*𝑜*3)  /  ∑3*𝑖*=1exp(*𝑜* *𝑖*).

容易看出*𝑦*̂1+*𝑦*̂2+*𝑦*̂3=1且0≤*𝑦*̂1,*𝑦*̂2,*𝑦*̂3≤1，因此*𝑦*̂1,*𝑦*̂2,*𝑦*̂3是一个合法的概率分布。

此外，argmax *𝑜*𝑖*=argmax 𝑦*̂*𝑖*,  因此softmax运算不改变预测类别输出。

改为矩阵形式亦同。

#### 交叉熵损失函数

交叉熵  ：*H*(**X**)=−*i*=1∑*n**P*(*x**i*)log(*P*(*x**i*)))(**X**=*x*1,*x*2,*x*3...,*x**n*)

公式复制来很奇怪，其实就是 x log x‘ 的和的相反数

当然，损失函数就是求各样本的交叉熵平均数。

注意，其中的 x 是向量中非零即一的元素，不是离散数值 x’ 。即，交叉熵只关心正确类别的预测概率，可以简化为-logx‘。当然，如果一个样本中包含多个物体，就不能做这步简化。

#### 模型评价

在训练好softmax回归模型后，给定任一样本特征，就可以预测每个输出类别的概率。通常，我们把预测概率最大的类别作为输出类别。如果它与真实类别（标签）一致，说明这次预测是正确的。可以使用准确率（accuracy）来评价模型的表现。它等于正确预测数量与总预测数量之比。

好吧，到这为止和gluon毫无关系。。。后续的代码会有的。。。

