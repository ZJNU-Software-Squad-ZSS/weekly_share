# 机器学习

 1. make decision
 2. going left/right  →discrete （离散的）
 3. increase/decrease → continuous（连续的）

# Continuous Prediction(连续值预测）

 - F*n* :x→y 
 - x:input data
 - F(x):prediction
 - y:real data,ground-truth
 - 有一个函数Fn，输入X后输出的F(x)与真实值y接近


# Linear Equation（线性方程）
例如一个线性方程 
 - y=w*x+b
 		
根据两组数据如下： 		
     1. 1.567=w+b
     2. 3.043=w*2+b
     
可得到w，b的值		
     - w=1.477
     - b=0.089

这种情况为 Closed Form Solution(闭式解)

但是基本不存在这种情况，因为模型的不确定性和数据的偏差

## With Noise？（高斯噪声）

 - y=w*x+b+ε
 - ε~N（0，1）
因为ε的存在，所以我们不能只观察两组样本数据，因为会有很大的随机性。

## For example
![在已知w，b的情况下，选取100个样本点后的图](https://img-blog.csdnimg.cn/20190909150736856.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwODAyOTU2,size_16,color_FFFFFF,t_70)
此图为在已知w（1.477），b（0.089）的情况下，选取100个样本点后的图，即y=1.477x+0.089+ε
随后我们将函数忘掉。
对于这个函数图像，我们可以有多种猜想：

 - 一次函数分布（蓝色）
 - 二次函数分布（黑色）
 - 更复杂的分布（棕色）
 ![在这里插入图片描述](https://img-blog.csdnimg.cn/2019090915144579.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwODAyOTU2,size_16,color_FFFFFF,t_70)
接下来我们要求解w和b参数
## Find w' b'
loss函数：
 $loss= \sum_i(w*x_i+b-y_i)^2$ 
当loss函数越来越小的时候，说明我们的w’和b’越来越精确。
现在将问题转化为求loss函数的极小值
## Gradient Descent(梯度下降）
我们用梯度下降方法寻找loss的最小值
![梯度函数](https://img-blog.csdnimg.cn/20190909160910423.png)此公式的意义是：J是关于Θ的一个函数，我们当前所处的位置为Θ0点，要从这个点走到J的最小值点，首先我们先确定前进的方向，也就是梯度的反向，然后走一段距离的步长，也就是α，走完这个段步长，就到达了Θ1这个点！
![在这里插入图片描述](https://img-blog.csdnimg.cn/20190909161031839.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQwODAyOTU2,size_16,color_FFFFFF,t_70)
当$\theta_1$基本没有什么变化时，说明已经到达最小值。
$w'=w-lr*\frac {\partial loss}{\partial w}$
$b'=b-lr*\frac {\partial loss}{\partial b}$

## linear regression(线性回归）

 - Linear regression(连续值）
 - Logistic regression（二分）
 - Classification（离散值）


## 核心代码

计算误差值：

```
def compute_error_for_line_given_points(b,w,points):
    totalError = 0
    for i in range(0,len(points)):
        x=points[i,0]
        y=points[i,1]
        totalError +=((w*x+b)-y)**2
    return totalError/float(len(points))
```
Find w，b

```
def step_gradient(b_current,w_current,points,learningRate):
    b_gradient = 0
    w_gradient = 0
    N= float(len(points))
    #loss对w，b求偏导
    for i in range(0,len(points)):
        x=points[i,0]
        y=points[i,1]
        b_gradient += (2/N)*(w_current*x+b_current-y)
        w_gradient += (2/N)*(w_current*x+b_current-y)*x
    #w'=w-lr*gradient
    new_b=b_current-learningRate*b_gradient
    new_w=w_current-learningRate*w_gradient
    return [new_b,new_w]
```
