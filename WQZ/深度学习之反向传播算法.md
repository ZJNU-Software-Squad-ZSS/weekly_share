## 深度学习之反向传播算法

反向传播算法（正是用来求这个复杂到爆的梯度的）

- <u>梯度向量每一项的大小是在告诉大家，代价函数对于每个参数有多敏感</u>，比如说你走了一段我讲的过程，计算了负梯度，对应这条线上这个权重的一项等于3.2，而对应这条边上的一项等于0.1，第一个权重对代价函数的值有32倍的影响力如果你稍微改变一下第一个权重，它对代价值造成的变化就是改变第二个权重同等大小下的32倍

- 每一个训练样本 会对权重偏置的调整造成怎样的影响，因为代价函数牵扯到对成千上万个训练样本的代价取平均值，所以我们调整每一步梯度下降用的权重偏置，也会基于所有的训练样本，原理上是这么说的，但为了计算效率，之后我们会讨个巧，从而不必每一步都非得要计算所有的训练样本

- 现在我们没有训练前所得到的10个随机激活值，要通过调整权重偏置进行调整，并且<u>变动的大小应该与现在值与目标值之间的差呈正比</u>，举个例子，增加数字“2”神经元的激活值，就应该比减少数字“8”神经元的激活值来得重要（“2”激活值0.2到1差0.8；“8”激活值0.2到0差0.2），因为后者已经很接近它的目标了，我们进一步来观察一下这个神经元（“2”），我们要让这里面的激活值变大，还记得这个激活值是把前一层所有激活值的加权和加上一个偏置（0.2=w0+w1+···+w(n-1)*a(n-1)+b），再通过sigmoid ReLU之类的挤压函数最后算出来的，所以<u>要增加这个激活值，我们有三条大路可走，一增加偏置，二增加权重，或者改变上一层的激活值</u>

- 先来看如何调整权重，各个权重它们的影响力各不相同，连接前一层最亮的神经元的权重，影响力也最大，因为这些权重会与大的激活值相乘，所有至少对于这个训练样本而言，增加了这几个权重值（前一层亮的神经元与这个神经元之间的权重连线），对最终代价函数造成的影响，比增加暗淡的神经元的权重影响要大上好多倍

- 当我们说梯度下降的时候，<u>我们并不只看每个参数是该增大还是减小，我们还看改哪个参数的性价比最高</u>

- *“赫布理论”总结起来就是“一同激活的神经元关联在一起”

- 这里权重的最大增长即连接变得更强的部分，就会发生在已经最活跃的神经元（前一层最亮的和这个神经元的权重连线）和想要更多激发的神经元之间，可以说看见一个2时激发的神经元，会和“想到一个2”时激发的神经元联系地更紧密

- 第三个方法：改变前一层的激活值，更具体地说，如果所有正权重连接的神经元更亮，所有 负权重连接的神经元更暗的话，那么数字2的神经元就会更强烈地激发，和改权重的时候类似，我们想造成更大的影响，就要依据对应权重的大小，对激活值做出呈比例的改变，当然，我们并不能直接改变激活值，<u>我们手头只能控制权重和偏置</u>

- 回过来我们不但要使“2”变大，还要调整其他神经元的激活值变小，但每个神经元对于如何改变倒数第二次，都有各自的想法；所以，我们会把数字2神经元的期待和别的输出神经元的期待全部加起来，作为如何改变倒数第二层神经元的指示，这些期待变化不仅是对应的权重的倍数，也是每个神经元激活值改变量的倍数，这其实就是在实现“反向传播”的理念了，我们把所有期待的改变加起来，就得到了一串对倒数第二层改动的变化量，有了这些，我们可以重复这些过程，改变影响倒数第二层神经元激活值的相关参数，一层层循环到第一层

- 我们前面只关注了那个“2”的要求，如果在这样所有图像都会被分类成是“2”，所以你要对其他所有的训练样本，同样地过一遍反向传播，记录下每一个样本想怎样修改权重与偏置，最后再取一个平均值，这里一系列的权重偏置的平均微调大小，不严格地说就是上期视频提到的代价函数的负梯度，至少是其标量的倍数，这里的不严格指的是我还没有准确地解释如何量化这些微调

  |        | 2     | 5     | 9     | ···· | 整行取平均 |
  | ------ | ----- | ----- | ----- | ---- | ---------- |
  | W0     | -0.08 | +0.02 | -0.14 |      | -0.08      |
  | W1     | -0.11 | +0.11 | +0.05 |      | +0.12      |
  | ····   |       |       |       |      |            |
  | W13000 | +0.13 | +0.08 | +0.04 |      | +0.04      |

  -▽C（w1，w2，···，w13000）={-0.08，+0.12，····，+0.04}

  

- 实际操作中，如果梯度下降的每一步，都用上每一个训练样本来计算的话，那么花的时间就太长了，所有我们一般会这么做，<u>首先把训练样本打乱，然后分成很多组mininbatch，每个minibatch就当包含100给训练样本好了，然后你算出这个minibatch下降的一步</u>，这不是代数函数真正的梯度，毕竟计算真实梯度得用上所有的样本而非这个子集，所有这也不是下山最高效的一步，然而每个minibatch都会给你一个不错的近似，而且更重要的是，你的计算量会减轻不少，这个技巧就叫做“随机梯度下降”

- 反向传播算法算的是单个训练样本想怎样修改权重与偏置，不仅是说每个参数应该变大还是变小，还包括了这些变化的比例是多大，才能最快地降低代价

- 反向传播算法在内所有包括神经网络在内的机器学习，要让它们工作，我们需要大量的训练数据，我们用的这个手写数字的范例之所以方便，是因为存在着一个MNIST数据库

- 所有机器学习领域的人，最熟悉的一个难关，莫过于获取标记好的训练数据