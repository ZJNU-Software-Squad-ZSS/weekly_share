## 深度学习之梯度下降法

**介绍梯度下降的思想（不仅是神经网络学习的基础，机器学习中的很多其他技术也，基于这个方法**）

**继续分析这个网络**

- 我们想要这么一种算法，你可以给这个网络看一大堆训练数据，其中包括一堆不同的手写数字图像以及它们代表哪个数字的标记，<u>算法会调整这13000个权重和偏置值，以提高网络对训练数据的表现</u>。
- 我们希望这种分层结构可以让它举一反三，识别训练数据之外的图像训练好网络后，我们给它更多以前从未见过的带标记的数据，作为测试，你就能看到它对这些新图像进行分类的准确度
- 机器学习更像是一道微积分习题，我们实际上是在找某个函数的最小值，从概念上讲，我们认为每个神经元都与上一层的所有神经元相连接，决定其激活值的加权和中的权重，有点像是那些连接的强弱，而偏置值则表明神经元是否更容易被激活
- 在一开始，我们会完全随机地初始化所有的权重和偏置值，可想而知，这个网络对于给定的训练示例，会表现得非常糟糕，毕竟它只是在做一些随机的判断，这时你就要定义一个“代价函数”来告诉电脑，“错误了，正确输出的激活值应该基本上是0，只有这个神经元是1”，用数学语言讲，你<u>要将每个垃圾输出激活值，与你想要的值之间的差的平方加起来（0.43-0.00）^2+，我们称之为训练单个样本的“代价”</u>,
- 注意一下，<u>网络能对图像进行正确的分类时，这个平方和就比较小</u>，但如果网络糊里糊涂找不到北的话，这个平方和就很大，你接下来就要考虑手头上几万个训练样本中，代价的平均值，而我们就要用这个平均代价来评价这个网络有多糟
- 要记得网络本身不过是个函数，有784给输入值，即像素的灰度，最后输出值是10个数字，而所有的权重和偏置值可以说就组成了这个函数的参数，
- 而代价函数还要再抽象一层，这13000多个权重和偏置值作为它的参数，它输出的是单个数值，来表示这些权重和偏置值有多差劲，而代价函数取决于网络对于上万个训练数据的综合表现，你只告诉电脑它的表现有多糟糕还不够，你还得告诉它，怎么改变这些权重和偏置值，有能有进步
- 为了简化问题，我们先不去想一个有13000个变量的函数C(w1,w2,···,w13000)，而先考虑简单的一元函数C(w)，只有一个输入变量，只输出一个数字,要怎么找输入值x，使得函数值最小化呢？微积分里dc/dw(w)=0就可以直接算出最小值了，但当函数很复杂，就不一定能写出来了，而对于13000元超复杂的代价函数，不肯定做到，<u>一个更灵活的技巧是先随便挑一个输入值，然后考虑向左还是向右走，函数才会变小</u>，准确地说，如果你<u>找到了函数在这里的斜率，那斜率为正就向左走，斜率为负就向右走</u>，在每一个点上都这样子重复，计算新斜率，再适当地走一小步的话，你就会<u>逼近函数的某个局部最小值</u>，你可以想象一个小球滚下山坡，由于不知道一开始输入值在哪里，最后你可能会落到许多不同的坑里，而且无法保证你落到的局部最小值就是代价函数可能达到的全局最小值，我们的神经网络也会遇到相同的问题，值得一提的是如果<u>每步的大小和斜率成比例</u>，那么在最小值附件斜率会越来越平缓，每步会越来越小，这样可以防止掉头
- 输入空间可以想象成XY平面，代价函数是平面上方的曲面，现在我们不问函数的斜率，而应该问在输入空间内沿哪个方向走，才好让输出结果降得最快，<u>多元积分中，函数的梯度指出了函数的最陡增长方向，即按梯度方向走，函数值增长得最快，那么沿梯度的负方向走，函数值自然就降低得最快了</u>，这个梯度向量的长度就代表了这个最陡的斜坡到底有多陡
- 先计算梯度，在按梯度反方向走一小步下山，如此循环
- 想象把13000个权重偏置都放到一个列向量里，那么代价函数的负梯度也不过是个向量，负梯度正是指出了在这个大到爆炸的函数输入空间内，具体如何改变每一项参数，才可以让代价函数的值下降得最快，那么对于我们的代价函数，更新权重和偏置来降低代价函数的值，意味着输入训练集的每一份样本的输出，都更接近期待的真实结果，要注意的是这个代价函数取了整个训练集的平均值，所有最小化的意思是对所有样本得到的总体结果都会更好一点，
- 这个计算梯度的算法是神经网络的核心，我们叫做反向传播算法【BP】
- 当我们提到网络学习是，实质上是<u>让代价函数的值最小</u>
- 而为了达到这个结果，代价函数非常有必要是平滑的，这样我们才能每次挪一点点，最后找到一个局部最小值，这也顺便解释了，为什么人工神经元的激活值是连续的，而非直接沿袭生物学神经元那种二元式的要么激活要么非激活的取值模式
- <u>这种按照负梯度的倍数，不断调整函数的输入值的过程就叫做梯度下降法</u>，这是一种可以让你收敛到代价函数图中的某个“坑”里
- 负梯度中的每一项都告诉我们两件事，正负号很明显在告诉我们输入向量的这一项该调大还是调小，每一项的相对大小更告诉了我们改变哪个值的影响更大