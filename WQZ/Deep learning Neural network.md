### Deep learning :Neural network

##### 数字识别

卷积神经网络——>擅长图像识别

长短期记忆网络——>擅长语音识别

多层感知器MLP

神经（元）网络

**什么是神经元？神经元如何连接起来的？**

 **Function（函数） 它输入的是上一层所有的神经元的输出，其实整个神经网络就是一个函数**

- 图像中的每个像素，神经元中装着的数字代表对应像素的灰度值，0表示纯黑像素，1表示纯白像素，神经元中装的数值叫激活值，在一张图片中，激活值越大，那么那个神经元就点着越亮，28*28=784像素的图片构成构成了第一层的784个神经元，最后一层是0-9共10个数字，它们的激活值同理处在0到1之间，网络中间还有几层“隐含层”。

- 网络层如何工作：上一层的激活值将决定下一层的激活值。神经网络处理信息的核心机制正是：一层的激活值是通过怎么样的运算，算出下一层的激活值的。如果你在网络输入层的784个神经元处，输入了784个代表输入图像各像素的灰度值，那么这层激活值的图案会让下层的激活值产生某些特殊图案，再让再下层的产生特殊图案，最终在输出层得到某种结果，而输出层最亮的那个神经元就表示神经网络的“选择”。

- 将最后的数字分解成一个个小部分组成，再把小部分拆成更小的部分。（转化为抽象元素，一层层的抽丝剥茧）

- 要判断第二层一个神经元的图像：我们需要给这个神经元和第一层所有神经元间的每一条接线都赋上一个权重值，然后我们拿起第一层所有的激活值和它们对应权重值一起，算出它们的加权和（w1*a1+w2*a2+w3*a3+····+wn*an）用函数把所得值挤压到0到1的区间内（sigmoid函数），加一个偏置-10，当加权和大于10时，激发才有意义；

- 第二层的每一个神经元都会和第一次全部的784个神经元相连接，每一个的784个接线上都带着一个权重，而且每个神经元都会在计算自己的加权和后加上自己的偏置，再通过sigmoid压缩输出自己的结果。

- <u>学习如何找到正确的权重和偏置，让它正确地解决问题</u>

- sigmoid函数已经过时了，ReLu（线性整流函数）更加适合训练，ReLU(a)=max(0,a),有一个阈值，没超过这个阈值，就不激发，输出0，超过一个阈值的时候，ReLU就和恒定函数一样。

  



