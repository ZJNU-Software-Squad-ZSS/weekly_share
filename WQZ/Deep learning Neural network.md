### Deep learning :Neural network

##### 数字识别

卷积神经网络——>擅长图像识别

长短期记忆网络——>擅长语音识别

多层感知器MLP

神经（元）网络

**什么是神经元？神经元如何连接起来的？**

 **Function（函数） 它输入的是上一层所有的神经元的输出，其实整个神经网络就是一个函数**

- 图像中的每个像素，神经元中装着的数字代表对应像素的灰度值，0表示纯黑像素，1表示纯白像素，神经元中装的数值叫激活值，在一张图片中，激活值越大，那么那个神经元就点着越亮，28*28=784像素的图片构成构成了第一层的784个神经元，最后一层是0-9共10个数字，它们的激活值同理处在0到1之间，网络中间还有几层“隐含层”。

- 网络层如何工作：上一层的激活值将决定下一层的激活值。神经网络处理信息的核心机制正是：一层的激活值是通过怎么样的运算，算出下一层的激活值的。如果你在网络输入层的784个神经元处，输入了784个代表输入图像各像素的灰度值，那么这层激活值的图案会让下层的激活值产生某些特殊图案，再让再下层的产生特殊图案，最终在输出层得到某种结果，而输出层最亮的那个神经元就表示神经网络的“选择”。

- 将最后的数字分解成一个个小部分组成，再把小部分拆成更小的部分。（转化为抽象元素，一层层的抽丝剥茧）

- 要判断第二层一个神经元的图像：我们需要给这个神经元和第一层所有神经元间的每一条接线都赋上一个权重值，然后我们拿起第一层所有的激活值和它们对应权重值一起，算出它们的加权和（w1*a1+w2*a2+w3*a3+····+wn*an）用函数把所得值挤压到0到1的区间内（sigmoid函数），加一个偏置-10，当加权和大于10时，激发才有意义；

- 第二层的每一个神经元都会和第一次全部的784个神经元相连接，每一个的784个接线上都带着一个权重，而且每个神经元都会在计算自己的加权和后加上自己的偏置，再通过sigmoid压缩输出自己的结果。

- <u>学习如何找到正确的权重和偏置，让它正确地解决问题</u>

- sigmoid函数已经过时了，ReLu（线性整流函数）更加适合训练，ReLU(a)=max(0,a),有一个阈值，没超过这个阈值，就不激发，输出0，超过一个阈值的时候，ReLU就和恒定函数一样。

  

**介绍梯度下降的思想（不仅是神经网络学习的基础，机器学习中的很多其他技术也，基于这个方法**）

**继续分析这个网络**

- 我们想要这么一种算法，你可以给这个网络看一大堆训练数据，其中包括一堆不同的手写数字图像以及它们代表哪个数字的标记，<u>算法会调整这13000个权重和偏置值，以提高网络对训练数据的表现</u>。
- 我们希望这种分层结构可以让它举一反三，识别训练数据之外的图像训练好网络后，我们给它更多以前从未见过的带标记的数据，作为测试，你就能看到它对这些新图像进行分类的准确度
- 机器学习更像是一道微积分习题，我们实际上是在找某个函数的最小值，从概念上讲，我们认为每个神经元都与上一层的所有神经元相连接，决定其激活值的加权和中的权重，有点像是那些连接的强弱，而偏置值则表明神经元是否更容易被激活
- 在一开始，我们会完全随机地初始化所有的权重和偏置值，可想而知，这个网络对于给定的训练示例，会表现得非常糟糕，毕竟它只是在做一些随机的判断，这时你就要定义一个“代价函数”来告诉电脑，“错误了，正确输出的激活值应该基本上是0，只有这个神经元是1”，用数学语言讲，你<u>要将每个垃圾输出激活值，与你想要的值之间的差的平方加起来（0.43-0.00）^2+，我们称之为训练单个样本的“代价”</u>,
- 注意一下，<u>网络能对图像进行正确的分类时，这个平方和就比较小</u>，但如果网络糊里糊涂找不到北的话，这个平方和就很大，你接下来就要考虑手头上几万个训练样本中，代价的平均值，而我们就要用这个平均代价来评价这个网络有多糟
- 要记得网络本身不过是个函数，有784给输入值，即像素的灰度，最后输出值是10个数字，而所有的权重和偏置值可以说就组成了这个函数的参数，
- 而代价函数还要再抽象一层，这13000多个权重和偏置值作为它的参数，它输出的是单个数值，来表示这些权重和偏置值有多差劲，而代价函数取决于网络对于上万个训练数据的综合表现，你只告诉电脑它的表现有多糟糕还不够，你还得告诉它，怎么改变这些权重和偏置值，有能有进步
- 为了简化问题，我们先不去想一个有13000个变量的函数C(w1,w2,···,w13000)，而先考虑简单的一元函数C(w)，只有一个输入变量，只输出一个数字,要怎么找输入值x，使得函数值最小化呢？微积分里dc/dw(w)=0就可以直接算出最小值了，但当函数很复杂，就不一定能写出来了，而对于13000元超复杂的代价函数，不肯定做到，<u>一个更灵活的技巧是先随便挑一个输入值，然后考虑向左还是向右走，函数才会变小</u>，准确地说，如果你<u>找到了函数在这里的斜率，那斜率为正就向左走，斜率为负就向右走</u>，在每一个点上都这样子重复，计算新斜率，再适当地走一小步的话，你就会<u>逼近函数的某个局部最小值</u>，你可以想象一个小球滚下山坡，由于不知道一开始输入值在哪里，最后你可能会落到许多不同的坑里，而且无法保证你落到的局部最小值就是代价函数可能达到的全局最小值，我们的神经网络也会遇到相同的问题，值得一提的是如果<u>每步的大小和斜率成比例</u>，那么在最小值附件斜率会越来越平缓，每步会越来越小，这样可以防止掉头
- 输入空间可以想象成XY平面，代价函数是平面上方的曲面，现在我们不问函数的斜率，而应该问在输入空间内沿哪个方向走，才好让输出结果降得最快，<u>多元积分中，函数的梯度指出了函数的最陡增长方向，即按梯度方向走，函数值增长得最快，那么沿梯度的负方向走，函数值自然就降低得最快了</u>，这个梯度向量的长度就代表了这个最陡的斜坡到底有多陡
- 先计算梯度，在按梯度反方向走一小步下山，如此循环
- 想象把13000个权重偏置都放到一个列向量里，那么代价函数的负梯度也不过是个向量，负梯度正是指出了在这个大到爆炸的函数输入空间内，具体如何改变每一项参数，才可以让代价函数的值下降得最快，那么对于我们的代价函数，更新权重和偏置来降低代价函数的值，意味着输入训练集的每一份样本的输出，都更接近期待的真实结果，要注意的是这个代价函数取了整个训练集的平均值，所有最小化的意思是对所有样本得到的总体结果都会更好一点，
- 这个计算梯度的算法是神经网络的核心，我们叫做反向传播算法【BP】
- 当我们提到网络学习是，实质上是<u>让代价函数的值最小</u>
- 而为了达到这个结果

