## 深度学习之梯度下降法

**介绍梯度下降的思想（不仅是神经网络学习的基础，机器学习中的很多其他技术也，基于这个方法**）

**继续分析这个网络**

- 我们想要这么一种算法，你可以给这个网络看一大堆训练数据，其中包括一堆不同的手写数字图像以及它们代表哪个数字的标记，<u>算法会调整这13000个权重和偏置值，以提高网络对训练数据的表现</u>。
- 我们希望这种分层结构可以让它举一反三，识别训练数据之外的图像训练好网络后，我们给它更多以前从未见过的带标记的数据，作为测试，你就能看到它对这些新图像进行分类的准确度
- 机器学习更像是一道微积分习题，我们实际上是在找某个函数的最小值，从概念上讲，我们认为每个神经元都与上一层的所有神经元相连接，决定其激活值的加权和中的权重，有点像是那些连接的强弱，而偏置值则表明神经元是否更容易被激活
- 在一开始，我们会完全随机地初始化所有的权重和偏置值，可想而知，这个网络对于给定的训练示例，会表现得非常糟糕，毕竟它只是在做一些随机的判断，这时你就要定义一个“代价函数”来告诉电脑，“错误了，正确输出的激活值应该基本上是0，只有这个神经元是1”，用数学语言讲，你<u>要将每个垃圾输出激活值，与你想要的值之间的差的平方加起来（0.43-0.00）^2+，我们称之为训练单个样本的“代价”</u>,
- 注意一下，<u>网络能对图像进行正确的分类时，这个平方和就比较小</u>，但如果网络糊里糊涂找不到北的话，这个平方和就很大，你接下来就要考虑手头上几万个训练样本中，代价的平均值，而我们就要用这个平均代价来评价这个网络有多糟
- 要记得网络本身不过是个函数，有784给输入值，即像素的灰度，最后输出值是10个数字，而所有的权重和偏置值可以说就组成了这个函数的参数，
- 而代价函数还要再抽象一层，这13000多个权重和偏置值作为它的参数，它输出的是单个数值，来表示这些权重和偏置值有多差劲，而代价函数取决于网络对于上万个训练数据的综合表现，你只告诉电脑它的表现有多糟糕还不够，你还得告诉它，怎么改变这些权重和偏置值，有能有进步
- 为了简化问题，我们先不去想一个有13000个变量的函数C(w1,w2,···,w13000)，而先考虑简单的一元函数C(w)，只有一个输入变量，只输出一个数字,要怎么找输入值x，使得函数值最小化呢？微积分里dc/dw(w)=0就可以直接算出最小值了，但当函数很复杂，就不一定能写出来了，而对于13000元超复杂的代价函数，不肯定做到，<u>一个更灵活的技巧是先随便挑一个输入值，然后考虑向左还是向右走，函数才会变小</u>，准确地说，如果你<u>找到了函数在这里的斜率，那斜率为正就向左走，斜率为负就向右走</u>，在每一个点上都这样子重复，计算新斜率，再适当地走一小步的话，你就会<u>逼近函数的某个局部最小值</u>，你可以想象一个小球滚下山坡，由于不知道一开始输入值在哪里，最后你可能会落到许多不同的坑里，而且无法保证你落到的局部最小值就是代价函数可能达到的全局最小值，我们的神经网络也会遇到相同的问题，值得一提的是如果<u>每步的大小和斜率成比例</u>，那么在最小值附件斜率会越来越平缓，每步会越来越小，这样可以防止掉头
- 输入空间可以想象成XY平面，代价函数是平面上方的曲面，现在我们不问函数的斜率，而应该问在输入空间内沿哪个方向走，才好让输出结果降得最快，<u>多元积分中，函数的梯度指出了函数的最陡增长方向，即按梯度方向走，函数值增长得最快，那么沿梯度的负方向走，函数值自然就降低得最快了</u>，这个梯度向量的长度就代表了这个最陡的斜坡到底有多陡
- 先计算梯度，在按梯度反方向走一小步下山，如此循环
- 想象把13000个权重偏置都放到一个列向量里，那么代价函数的负梯度也不过是个向量，负梯度正是指出了在这个大到爆炸的函数输入空间内，具体如何改变每一项参数，才可以让代价函数的值下降得最快，那么对于我们的代价函数，更新权重和偏置来降低代价函数的值，意味着输入训练集的每一份样本的输出，都更接近期待的真实结果，要注意的是这个代价函数取了整个训练集的平均值，所有最小化的意思是对所有样本得到的总体结果都会更好一点，
- 这个计算梯度的算法是神经网络的核心，我们叫做反向传播算法【BP】
- 当我们提到网络学习是，实质上是<u>让代价函数的值最小</u>
- 而为了达到这个结果，代价函数非常有必要是平滑的，这样我们才能每次挪一点点，最后找到一个局部最小值，这也顺便解释了，为什么人工神经元的激活值是连续的，而非直接沿袭生物学神经元那种二元式的要么激活要么非激活的取值模式
- <u>这种按照负梯度的倍数，不断调整函数的输入值的过程就叫做梯度下降法</u>，这是一种可以让你收敛到代价函数图中的某个“坑”里
- 负梯度中的每一项都告诉我们两件事，正负号很明显在告诉我们输入向量的这一项该调大还是调小，每一项的相对大小更告诉了我们改变哪个值的影响更大
- 我们的网络中调整其中一个权重，对代价函数的影响，比调整别的权重大得多，对训练数据来说，其中某些连线就是更加的重要，所有，你再看到这个复杂到爆炸的代价函数它的梯度向量时，就可以把它理解为各个权重偏置的相对重要度，标记出了改变哪个参数，性价比最高，这也是理解的另一种方向；举个例子，假设有个输入两个变量的二元函数C(x,y)=3/2 x^2 + 1/2 y^2，你算出这个函数在某个点的梯度是【3，1】，一种解读就是你站在这个点顺着这个梯度的方向移动，函数值增加得最快,另一种解读是第一个变量的重要性是第二个变量的3倍，也就是说，起码在这一块取值区域内，改变x的值会造成更大的影响
- 那么当你随机初始化权重和偏置，并通过梯度下降法调整了很多次参数之后，神经网络来识别它从未见过的图像时，表现并不赖
- 一开始我们介绍这种结构期望的是它的第二层能识别出短边，第三层能把短边拼成圈或者长笔画，最后把这些部件拼成数字，而对于神经网络完全不是这样做的。还记得上节课中我们可以把第一层所有神经元和第二层某一神经元间所有的权重，可以表示为第二层神经元所识别的一个像素图案吗，而实际运用中，第一层和第二层之间的权重，与其说它们能识别出各种散落的短边，它们看起来其实没有什么规律，中间只有一些松散的图案，感觉就像在如深海一般13000维的参数空间中，我们的网络找到了一个还不错的局部最小值住了下来，尽管它可以成功对绝大多数图像做分类，但它并不会如期望一般识别出图像中的各种图案，
- 换句话讲，虽然它能很好的识别数字，但它也并不知道自己如何写数字，究其原因，很大程度上是因为它的训练被限定在了很窄的框架内，试想一下神经网络的第一视角，从它的角度看，整个宇宙中都是由小网格内清晰定义的静止数字所组成，而它的代价函数则只会促使它对最后的判断有绝对的自信
- 学习资料：Michael Nielson 深度学习